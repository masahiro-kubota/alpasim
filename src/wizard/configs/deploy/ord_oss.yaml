# @package _global_
# Config for running the OSS stack on ORD resources.
# Use the cmd line arg "+deploy=ord_oss" when calling the wizard or sbatch submit.sh

defaults:
  - /stable_manifest/oss
  - _self_

defines:
  filesystem: "/lustre/fs12/portfolios/av/projects/av_alpamayo_sim/.cache"
  nre_cache_size: 5  # Because below we have 2 sensorsim instances on each GPU
  physics_cache_size: 6  # should match or exceed concurrent unique scenes

eval.num_processes: 32

wizard:
    run_method: "SLURM"

services:
  sensorsim:
    environments:
      - OMP_NUM_THREADS=1

      # leave some room for traffic sim if enabled in sub-configs
      - PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.7

    replicas_per_container: 4
    gpus: [0, 1, 2, 3]


  driver:
    replicas_per_container: 8
    gpus: [4, 5, 6, 7]

  physics:
    replicas_per_container: 4
    gpus: [4, 5, 6, 7]

  controller:
    replicas_per_container: 16
    gpus: null

  trafficsim:
    replicas_per_container: 1
    gpus: [0, 1]

runtime:
  # 64 endpoints / 8 workers = 8 endpoints per worker
  nr_workers: 8
  endpoints:
    # scaling the number of rollouts assigned to each endpoint to add up
    # # instances x # rollouts = number of rollouts
    # SENS: 4 (nr_gpus)  x 4 (replicas_per_container) x 4 (n_concurrent_rollouts) = 64
    # DRIV: 4 (nr_gpus)  x 8 (replicas_per_container) x 2 (n_concurrent_rollouts) = 64
    # PHYS: 4 (nr_gpus)  x 4 (replicas_per_container) x 4 (n_concurrent_rollouts) = 64
    # CONT: 1 (nr_gpus)  x 16 (replicas_per_container) x 4 (n_concurrent_rollouts) = 64
    sensorsim:
      n_concurrent_rollouts: 4

    driver:
      n_concurrent_rollouts: 2
      skip: false

    physics:
      n_concurrent_rollouts: 4
      skip: false

    controller:
      n_concurrent_rollouts: 4
      skip: false

    trafficsim:
      skip: true
