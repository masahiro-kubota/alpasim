# @package _global_
# Config for running on ORD, points to artifacts and caches managed by alpasim team
# Use using the cmd line arg "+deploy=ord" when calling the wizard or sbatch submit.sh

defaults:
  - /stable_manifest/internal
  - _self_

defines:
  filesystem: "/lustre/fs12/portfolios/av/projects/av_alpamayo_sim/.cache"
  nre_cache_size: 8
  physics_cache_size: 16  # should match or exceed concurrent unique scenes

wizard:
    run_method: "SLURM"

services:
  sensorsim:
    environments:
      - OMP_NUM_THREADS=1

      # leave some room for traffic sim if enabled in sub-configs
      - PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.7

    replicas_per_container: 1
    gpus: [0, 1, 2, 3]


  driver:
    environments:
      - PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.5 # 40gb should be ample

    gpus: [4, 5, 6, 7]
    replicas_per_container: 1

    volumes:
      # Base config
      - "${defines.alpackages}:/mnt/alpackages"
      - "${wizard.log_dir}:/mnt/output"

      # New mount for lustre
      - "/lustre:/lustre"

  physics:
    replicas_per_container: 1
    gpus: [2, 3, 4, 5, 6, 7]

  controller:
    replicas_per_container: 2
    gpus: null

  trafficsim:
    replicas_per_container: 1
    gpus: [0, 1]

runtime:
  endpoints:
    # scaling the number of rollouts assigned to each endpoint to add up
    # # instances x # rollouts = number of rollouts
    # 4          x 3         = 12
    # 4          x 3         = 12
    # 6          x 2         = 12
    # 2          x 6         = 12
    sensorsim:
      n_concurrent_rollouts: 3

    driver:
      n_concurrent_rollouts: 3

    physics:
      n_concurrent_rollouts: 2
      skip: false

    controller:
      n_concurrent_rollouts: 6
      skip: false

    trafficsim:
      skip: true
