# SPDX-License-Identifier: Apache-2.0
# Copyright (c) 2025 NVIDIA Corporation

"""
The main entrypoint to start simulations with alpasim.
"""

from __future__ import annotations

import argparse
import asyncio
import logging
import os
import random
import sys
from pathlib import Path
from uuid import uuid4

import yaml
from alpasim_grpc.v0.common_pb2 import Empty
from alpasim_runtime.autoresume import (
    find_num_complete_rollouts,
    remove_incomplete_rollouts,
)
from alpasim_runtime.config import (
    NetworkSimulatorConfig,
    SimulatorConfig,
    UserSimulatorConfig,
    typed_parse_config,
)
from alpasim_runtime.endpoints import get_service_endpoints
from alpasim_runtime.telemetry.plot_metrics import generate_metrics_plot
from alpasim_runtime.telemetry.utils import merge_metrics_files
from alpasim_runtime.validation import (
    gather_versions_from_addresses,
    validate_scenarios,
)
from alpasim_runtime.worker.ipc import RolloutJob
from alpasim_runtime.worker.pool import run_workers

import grpc

logger = logging.getLogger(__name__)


def parse_config(user_config_path: str, network_config_path: str) -> SimulatorConfig:
    """
    Parses the user-generated config and the networking config (usually autogenerated) and assembles
    them into a single config struct
    """
    user_config = typed_parse_config(user_config_path, UserSimulatorConfig)
    network_config = typed_parse_config(network_config_path, NetworkSimulatorConfig)

    return SimulatorConfig(user=user_config, network=network_config)


def get_run_name(log_dir: str) -> str:
    run_metadata_path = os.path.join(log_dir, "run_metadata.yaml")
    with open(run_metadata_path, "r") as f:
        run_metadata = yaml.safe_load(f)
    return run_metadata.get("run_name")


def create_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()

    # we split user and network config files because the latter is commonly auto-generated by kubernetes
    parser.add_argument("--user-config", type=str, required=True)
    parser.add_argument("--network-config", type=str, required=True)
    parser.add_argument("--usdz-glob", type=str, required=True)
    parser.add_argument(
        "--log-dir",
        type=str,
        required=True,
        help="Root directory for all simulation outputs (asl/, metrics/, txt-logs/)",
    )

    parser.add_argument(
        "--log-level",
        type=str,
        required=False,
        default="INFO",
        help="Python logging level (e.g. DEBUG, INFO, WARNING, ERROR)",
    )

    return parser


def build_job_list(config: SimulatorConfig, asl_dir: str) -> list[RolloutJob]:
    """Build list of jobs to execute, respecting autoresume settings."""
    jobs = []
    for scenario in config.user.scenarios:
        n_rollouts_to_dispatch = scenario.n_rollouts

        # Handle autoresume: skip already-completed rollouts
        if config.user.enable_autoresume:
            remove_incomplete_rollouts(asl_dir, scenario.scene_id)
            num_finished_rollouts = find_num_complete_rollouts(
                asl_dir, scenario.scene_id
            )
            if num_finished_rollouts > 0:
                logger.info(
                    f"Found {num_finished_rollouts} already completed rollouts for "
                    f"scene_id={scenario.scene_id}"
                )
                n_rollouts_to_dispatch -= num_finished_rollouts

        # Create jobs for remaining rollouts
        for _ in range(n_rollouts_to_dispatch):
            jobs.append(
                RolloutJob(
                    job_id=uuid4().hex,
                    scenario=scenario,
                    seed=random.randint(0, 2**32 - 1),
                )
            )

    logger.info("Built %d jobs to execute", len(jobs))
    return jobs


async def shutdown_services(config: SimulatorConfig) -> None:
    """Shut down all services by sending shutdown command to each unique address."""
    if not config.user.endpoints.do_shutdown:
        logger.info("Skipping service shutdown (do_shutdown=False)")
        return

    logger.info("Shutting down services...")

    service_endpoints = get_service_endpoints(config.network)

    for stub_class, addresses in service_endpoints.values():
        for address in addresses:
            try:
                channel = grpc.aio.insecure_channel(address)
                stub = stub_class(channel)
                await stub.shut_down(Empty(), timeout=5.0)
                await channel.close()
            except Exception as e:
                # Service may already be shut down or unreachable - that's OK
                logger.debug("Failed to shut down %s: %s", address, e)

    logger.info("Service shutdown complete")


async def run_simulation(args: argparse.Namespace) -> bool:
    """Main simulation orchestration."""
    config = parse_config(args.user_config, args.network_config)

    # Derive output directories from log_dir
    asl_dir = os.path.join(args.log_dir, "asl")
    metrics_dir = os.path.join(args.log_dir, "metrics")

    nr_workers = config.user.nr_workers

    # Validate nr_workers
    if nr_workers < 1:
        raise ValueError(f"nr_workers must be >= 1, got {nr_workers}")

    # Health check: probe all services to ensure they're ready before spawning workers.
    await gather_versions_from_addresses(
        config.network,
        timeout_s=config.user.endpoints.startup_timeout_s,
    )

    await validate_scenarios(config)
    jobs = build_job_list(config, asl_dir)

    if not jobs:
        logger.info("No jobs to run (all rollouts already complete or no scenarios).")
        return True

    try:
        results = await run_workers(config, args, jobs, args.log_dir)
    finally:
        await shutdown_services(config)

    # Check for failures
    success = all(r.success for r in results)
    if not success:
        failed = [r for r in results if not r.success]
        logger.error("%d jobs failed:", len(failed))
        for r in failed[:3]:
            logger.error("  Job %s: %s", r.job_id, r.error)
        if len(failed) > 3:
            logger.error("  ... and %d more", len(failed) - 3)

    # Merge metrics files
    merge_metrics_files(metrics_dir)

    # Generate metrics visualization plot
    output_path = generate_metrics_plot(
        metrics_path=Path(metrics_dir) / "metrics.prom",
        output_path=Path(args.log_dir) / "metrics_plot.png",
        run_name=get_run_name(args.log_dir),
    )
    logger.info("Generated metrics plot: %s", output_path)

    return success


if __name__ == "__main__":
    parser = create_arg_parser()
    args = parser.parse_args()

    logging.basicConfig(
        level=getattr(logging, args.log_level.upper(), None),
        format="%(asctime)s.%(msecs)03d %(levelname)s:\t%(message)s",
        datefmt="%H:%M:%S",
    )

    success = asyncio.run(run_simulation(args))
    logging.info("Alpasim finished.")

    sys.exit(0 if success else 1)
